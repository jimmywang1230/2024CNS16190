{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --upgrade --quiet  langchain langchain-community langchainhub langchain-openai langchain-chroma bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import getpass\n",
    "# import os\n",
    "# import openai\n",
    "\n",
    "\n",
    "# import dotenv\n",
    "\n",
    "# dotenv.load_dotenv()\n",
    "# OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "# openai.api_key = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.globals import set_verbose\n",
    "\n",
    "set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "from werkzeug.utils import secure_filename\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "import uuid\n",
    "import openai\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader, CSVLoader\n",
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain.chains import RetrievalQA, ConversationChain\n",
    "from langchain_community.llms import OpenAI\n",
    "from langchain_community.embeddings import OpenAIEmbeddings\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "openai.api_key = OPENAI_API_KEY\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "app = Flask(__name__)\n",
    "app.config['UPLOAD_FOLDER'] = 'uploads/'\n",
    "app.config['DATABASE_FOLDER'] = 'database/'\n",
    "ALLOWED_EXTENSIONS = {'pdf', 'csv'}\n",
    "\n",
    "if not os.path.exists(app.config['UPLOAD_FOLDER']):\n",
    "    os.makedirs(app.config['UPLOAD_FOLDER'])\n",
    "\n",
    "if not os.path.exists(app.config['DATABASE_FOLDER']):\n",
    "    os.makedirs(app.config['DATABASE_FOLDER'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowed_file(filename):\n",
    "    return '.' in filename and filename.rsplit('.', 1)[1].lower() in ALLOWED_EXTENSIONS\n",
    "\n",
    "def process_file(filepath):\n",
    "    global retrieval_chain, db\n",
    "\n",
    "    _, file_extension = os.path.splitext(filepath)\n",
    "    file_extension = file_extension.lower()\n",
    "\n",
    "    print(f\"Processing file: {filepath}\")\n",
    "    \n",
    "    if file_extension == '.pdf':\n",
    "        loader = PyPDFLoader(filepath)\n",
    "    elif file_extension == '.csv':\n",
    "        loader = CSVLoader(filepath, encoding='utf-8')\n",
    "    else:\n",
    "        print(f\"Unsupported file type: {filepath}\")\n",
    "        return\n",
    "\n",
    "    docs = loader.load_and_split()\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "    chunked_documents = text_splitter.split_documents(docs)\n",
    "\n",
    "    if db is None:\n",
    "        db = Qdrant.from_documents(chunked_documents, OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY), location=\":memory:\")\n",
    "    else:\n",
    "        db.add_documents(chunked_documents)\n",
    "\n",
    "    retriever = db.as_retriever()\n",
    "    retrieval_chain = RetrievalQA.from_chain_type(llm=llm, chain_type=\"stuff\", retriever=retriever)\n",
    "    print(\"Database initialized and retrieval chain set\")\n",
    "\n",
    "def load_all_files_from_database():\n",
    "    database_folder = app.config['DATABASE_FOLDER']\n",
    "    for filename in os.listdir(database_folder):\n",
    "        if allowed_file(filename):\n",
    "            filepath = os.path.join(database_folder, filename)\n",
    "            process_file(filepath)\n",
    "            print(f\"Processed file: {filename}\")\n",
    "    return 'All files in the database folder have been loaded and processed.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import bs4\n",
    "from langchain import hub\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load, chunk and index the contents of the blog.\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://lilianweng.github.io/posts/2023-06-23-agent/\",),\n",
    "    bs_kwargs=dict(\n",
    "        parse_only=bs4.SoupStrainer(\n",
    "            class_=(\"post-content\", \"post-title\", \"post-header\")\n",
    "        )\n",
    "    ),\n",
    ")\n",
    "docs = loader.load()\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "splits = text_splitter.split_documents(docs)\n",
    "vectorstore = Chroma.from_documents(documents=splits, embedding=OpenAIEmbeddings())\n",
    "\n",
    "# Retrieve and generate using the relevant snippets of the blog.\n",
    "retriever = vectorstore.as_retriever()\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
    "llm = ChatOpenAI(api_key=OPENAI_API_KEY, model=\"gpt-4o\", request_timeout=60)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'retriever' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 16\u001b[0m\n\u001b[0;32m      4\u001b[0m contextualize_q_system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mGiven a chat history and the latest user question \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124mwhich might reference context in the chat history, formulate a standalone question \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124mwhich can be understood without the chat history. Do NOT answer the question, \u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124mjust reformulate it if needed and otherwise return it as is.\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m      8\u001b[0m contextualize_q_prompt \u001b[38;5;241m=\u001b[39m ChatPromptTemplate\u001b[38;5;241m.\u001b[39mfrom_messages(\n\u001b[0;32m      9\u001b[0m     [\n\u001b[0;32m     10\u001b[0m         (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, contextualize_q_system_prompt),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     13\u001b[0m     ]\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m history_aware_retriever \u001b[38;5;241m=\u001b[39m create_history_aware_retriever(\n\u001b[1;32m---> 16\u001b[0m     llm, \u001b[43mretriever\u001b[49m, contextualize_q_prompt\n\u001b[0;32m     17\u001b[0m )\n",
      "\u001b[1;31mNameError\u001b[0m: name 'retriever' is not defined"
     ]
    }
   ],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "contextualize_q_system_prompt = \"\"\"Given a chat history and the latest user question \\\n",
    "which might reference context in the chat history, formulate a standalone question \\\n",
    "which can be understood without the chat history. Do NOT answer the question, \\\n",
    "just reformulate it if needed and otherwise return it as is.\"\"\"\n",
    "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", contextualize_q_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "history_aware_retriever = create_history_aware_retriever(\n",
    "    llm, retriever, contextualize_q_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "\n",
    "qa_system_prompt = \"\"\"You are a helpful assistant that understands CNS16190, EN303645, and ETSI TS 103 701. \\\n",
    "    Use the following pieces of context to answer the question at the end. \\\n",
    "    If you don't know the answer, just say that you don't know, don't try to make up an answer.\\\n",
    "    When you are asked to generate a test scenario, you should use three things to generate a test scenario: \n",
    "    1. given provision number and description \n",
    "    2. provided feature that corresponds to provision description\n",
    "    3. Find the test scenario in the ETSI TS 103 701 that corresponds to the provision number and description\n",
    "    \n",
    "    Context: {context}\n",
    "    \"\"\"\n",
    "qa_prompt = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", qa_system_prompt),\n",
    "        MessagesPlaceholder(\"chat_history\"),\n",
    "        (\"human\", \"{input}\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
    "\n",
    "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "chat_history = []\n",
    "\n",
    "question = \"What is Task Decomposition?\"\n",
    "ai_msg_1 = rag_chain.invoke({\"input\": question, \"chat_history\": chat_history})\n",
    "chat_history.extend([HumanMessage(content=question), ai_msg_1[\"answer\"]])\n",
    "\n",
    "second_question = \"What are common ways of doing it?\"\n",
    "ai_msg_2 = rag_chain.invoke({\"input\": second_question, \"chat_history\": chat_history})\n",
    "\n",
    "print(ai_msg_2[\"answer\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
